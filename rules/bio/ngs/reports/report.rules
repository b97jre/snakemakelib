# -*- snakemake -*-
import os
from collections import namedtuple
from mako.template import Template
from snakemake.utils import report
from snakemakelib.config import update_sml_config, sml_rules_path, get_sml_config
from snakemakelib.utils import utc_time
from snakemakelib.report.picard import PicardMetrics, AlignMetrics, InsertMetrics, HsMetrics, DuplicationMetrics, combine_metrics
import matplotlib
matplotlib.use('Agg')
from pylab import *
import matplotlib.pyplot as plt
import numpy as np

# Templates
picard_qc_report = Template("""
Project summary
=============================

:Project: ${project_name}
:Application: ${application}

Sample QC summary
------------------

.. csv-table:: Sample QC summary. Columns show sample name, total number of reads, percent aligned reads, percent duplication, mean insert size, mean coverage over target regions, percent sequenced bases that have aligned to target, followed by the percent bases in target regions covered at 10X and 30X
   :class: docutils
   :file: report/picardmetricssummary.csv
   :header-rows: 1

QC Metrics
----------

Sequence statistics
^^^^^^^^^^^^^^^^^^^

.. figure:: ${seqstats}


Alignment metrics
^^^^^^^^^^^^^^^^^

.. figure:: ${alnmet}

Duplication metrics
^^^^^^^^^^^^^^^^^^^^

.. figure:: ${dupmet}

Insert metrics
^^^^^^^^^^^^^^^^^^^^

.. figure:: ${insmet}

Target metrics
^^^^^^^^^^^^^^^^^^^^

.. figure:: ${targetmet}


.. figure:: ${target2dup}


Hybridization metrics
^^^^^^^^^^^^^^^^^^^^^

.. figure:: ${hsmet}


Sample-based hybridization metrics
----------------------------------

% for f in hsmetsub:
.. figure:: ${f}
   :align: center

% endfor


""")

gatk_eval_report = Template("""

:Project: ${project_name}
:Application: ${application}
:Input: ${input}


Sample-based variant summary
-----------------------------

Table 1 shows a variant summary stratified by sample. 

Results from CompOverlap
-------------------------

.. figure:: ${variants_per_sample}

.. figure:: ${known_site_freq}

.. figure:: ${dbsnp_concordance_known}


Results from CountVariants
---------------------------

.. figure:: ${nSNP}

.. figure:: ${nIns}

.. figure:: ${nDel}

.. figure:: ${nComp}

.. figure:: ${nMNP}

.. figure:: ${nHets}

.. figure:: ${nHomRef}

.. figure:: ${nHomVar}

.. figure:: ${nNoCalls}

.. figure:: ${nSingletons}

Results from TiTvVariantEvaluator
-----------------------------------

.. figure:: ${TiTv}
""")

rule_report = Template("""
:Project: ${project_name}
:Application: ${application}

Rule graphs and DAGs
---------------------

% for r in rulegraphs:

${r}
${"^" * (len(r) + 1)}

.. figure:: ${r}
   :align: center

   Rulegraph for ${r}

% endfor


""")


# Start by including the general snakefile
include: os.path.join(sml_rules_path(), 'bio/ngs', 'settings.rules')
include: os.path.join(sml_rules_path(), 'bio/ngs/qc', 'picard.rules')
include: os.path.join(sml_rules_path(), 'bio/ngs/tools', 'gatk.rules')

def find_sample_metrics(wildcards):
    """Simple sample metrics finding function"""
    extensions = [report_cfg['picard']['alnmetrics'], 
                  report_cfg['picard']['dupmetrics'], 
                  report_cfg['picard']['hsmetrics'], 
                  report_cfg['picard']['insmetrics']]
    metrics = []
    if cfg['bio.ngs.settings']['samples']:
        for s in cfg['bio.ngs.settings']['samples']:
            metrics += [os.path.join(workflow._workdir, s, "".join([s, report_cfg['picard']['bamlabel'], ext])) for ext in extensions]
    return metrics

config_default = { 
    'bio.ngs.report' : {
        'project_id' : "",
        'project_name' : "",
        'basedir' : os.curdir,
        'graphicsdir' : 'report/grf',
        'reportdir' : 'report',
        'application':"",
        'date':utc_time(),
        'rulegraphs' : [], 
        'picard' : {
            'columns' : ["ZERO_CVG_TARGETS_PCT", "PCT_TARGET_BASES_2X", "PCT_TARGET_BASES_10X", "PCT_TARGET_BASES_20X", "PCT_TARGET_BASES_30X", "PCT_TARGET_BASES_40X", "PCT_TARGET_BASES_50X", "PCT_TARGET_BASES_100X"],
            'hticks' : ["0X", "2X", "10X", "20X", "30X", "40X", "50X", "100X"],
            'bamlabel' : '.sort.merge.rg',
            'alnmetrics' : '.dup.align_metrics',
            'dupmetrics' : '.dup_metrics',
            'hsmetrics' : '.dup.hs_metrics',
            'insmetrics' : '.dup.insert_metrics',
            'inputfun' : find_sample_metrics,
            'summarycolumns' : ["SAMPLE", "TOTAL_READS", "PCT_PF_READS_ALIGNED", "PERCENT_DUPLICATION", "MEAN_INSERT_SIZE", "MEAN_TARGET_COVERAGE", "PCT_ON_TARGET", "PCT_TARGET_BASES_10X", "PCT_TARGET_BASES_30X"],
            'summarycolumn_names' : ['sample', 'total', '%aligned', '%dup', 'insert', 'meancov', '%ontarget', '10X', '30X'],
        },
    },
}

update_sml_config(config_default)

cfg = get_sml_config()

report_cfg = get_sml_config('bio.ngs.report')

rule report_generate_qc_plots:
    """Generate qc plots"""
    input: report_cfg['picard']['inputfun']
    output: seqstats = os.path.join(report_cfg['graphicsdir'], 'seqstats.png'),
            alnmet = os.path.join(report_cfg['graphicsdir'], 'alnmet.png'),
            dupmet = os.path.join(report_cfg['graphicsdir'], 'dupmet.png'),
            insmet = os.path.join(report_cfg['graphicsdir'], 'insmet.png'),
            targetmet = os.path.join(report_cfg['graphicsdir'], 'targetmet.png'),
            target2dup = os.path.join(report_cfg['graphicsdir'], 'target2dup.png'),
            hsmet = os.path.join(report_cfg['graphicsdir'], 'hsmet.png'),
            hsmetsub = [os.path.join(report_cfg['graphicsdir'], 'hsmetsub-' + str(x) + '.png') for x in range(0, math.ceil(len(cfg['bio.ngs.settings']['samples'])/9))],
            metricstable = os.path.join(report_cfg['reportdir'], 'picardmetrics.csv'),
            summarytable = os.path.join(report_cfg['reportdir'], 'picardmetricssummary.csv')
    run:
      """collect metrics and return PicardMetrics object"""
      samples = cfg['bio.ngs.settings']['samples']
      # Collect pm metrics and plot
      mlist =(list(
          zip(
              [AlignMetrics(filename=x).category() for x in input if x.endswith(report_cfg['picard']['alnmetrics'])],
              [InsertMetrics(filename=x) for x in input if x.endswith(report_cfg['picard']['insmetrics'])],
              [DuplicationMetrics(filename=x) for x in input if x.endswith(report_cfg['picard']['dupmetrics'])],
              [HsMetrics(filename=x) for x in input if x.endswith(report_cfg['picard']['hsmetrics'])]
          )
      ))
      pm = combine_metrics(mlist)
      pm.add_column('PCT_ON_TARGET', [str(100 * float(x)/float(y)) for (x,y) in zip(pm.x('ON_TARGET_BASES'), pm.y('PF_UQ_BASES_ALIGNED'))], **{'PCT_ON_TARGET' : ('3.2f', float)})
      pm.metrics['SAMPLE'] = samples
      # Sequence statistics plot
      sdup = [int(50 + 500 * x) for x in pm.x('PERCENT_DUPLICATION')]
      plt.scatter(pm.x('PCT_PF_READS_ALIGNED'), pm.y('TOTAL_READS'), s=sdup, alpha=0.5)
      plt.xlabel(r'Percent aligned', fontsize=14)
      plt.yscale('log', **{'basey':10})
      plt.xticks(arange(0,1.1,0.1), range(0,110,10))
      plt.ylabel(r'Read count', fontsize=14)
      plt.title("Sequence summary.\nPoint sizes correspond to duplication levels.", fontsize=14)
      plt.tight_layout()
      plt.savefig(output['seqstats'])
      plt.close()

      # Alignment metrics
      n = len(samples)
      plt.xlim(0, n+2)
      plt.xticks(range(1,n+1), [x for x in samples], rotation=90)
      plt.ylim(0,1)
      plt.yticks(arange(0,1.1,0.1), range(0,110,10))
      plt.plot(range(1,n+1), pm.x('PCT_PF_READS_ALIGNED'), "o")
      plt.xlabel(r'Sample', fontsize=14)
      plt.ylabel(r'Percent aligned', fontsize=14)
      plt.tight_layout()
      plt.savefig(output['alnmet'])
      plt.close()

      # Duplication metrics
      plt.xlim(0, n+2)
      plt.xticks(range(1,n+1), [x for x in samples], rotation=90)
      plt.ylim(0,1)
      plt.yticks(arange(0,1.1,0.1), range(0,110,10))
      plt.plot(range(1,n+1), pm.x('PERCENT_DUPLICATION'), "o")
      plt.xlabel(r'Sample', fontsize=14)
      plt.ylabel(r'Percent duplication', fontsize=14)
      plt.tight_layout()
      plt.savefig(output['dupmet'])
      plt.close()

      # Insert metrics
      plt.xlim(0, n+2)
      plt.xticks(range(1,n+1), [x for x in samples], rotation=90)
      plt.plot(range(1,n+1), pm.x('MEAN_INSERT_SIZE'), "o")
      plt.xlabel(r'Sample', fontsize=14)
      plt.ylabel(r'Mean insert size', fontsize=14)
      plt.tight_layout()
      plt.savefig(output['insmet'])
      plt.close()

      # Target metrics
      plt.xlim(0, n+2)
      plt.xticks(range(1,n+1), [x for x in samples], rotation=90)
      plt.plot(range(1,n+1), pm.x('PCT_ON_TARGET'), "o")
      plt.xlabel(r'Sample', fontsize=14)
      plt.ylabel(r'Percent on target', fontsize=14)
      plt.tight_layout()
      plt.savefig(output['targetmet'])
      plt.close()

      # Target metrics
      plt.plot(pm.x('PERCENT_DUPLICATION'), pm.y('PCT_ON_TARGET'), "o")
      plt.xlabel(r'Percent duplication', fontsize=14)
      plt.ylabel(r'Percent on target', fontsize=14)
      plt.tight_layout()
      plt.savefig(output['target2dup'])
      plt.close()
      
      # Hs metrics
      columns = report_cfg['picard']['columns']
      hticks = report_cfg['picard']['hticks']
      hsmetrics = pm[columns]

      # Hs boxplot metrics
      plt.ylim(0,1)
      plt.yticks(arange(0,1.1,0.1), range(0,110,10))
      plt.boxplot(np.array(hsmetrics.as_list()[1:]))
      plt.xticks(range(1,len(hticks)+1), [x for x in hticks])
      plt.savefig(output['hsmet'])
      plt.close()

      nsubplots = int(math.ceil(n/9))
      k = 0
      for i_subplot in range(0, nsubplots):
          f, axarr = plt.subplots(3, 3, sharex='col', sharey='row')
          for i in range(0, 3):
              for j in range(0, 3):
                  if k < n:
                      x = range(1, len(hticks) + 1)
                      axarr[i,j].plot(x, hsmetrics.as_list()[1:][k], "o")
                      axarr[i,j].set_xticks(x)
                      axarr[i,j].set_title(samples[k])
                      axarr[i,j].set_xlim(0, (len(hticks) + 1))
                      axarr[i,j].set_ylim(-0.05, 1.05)
                      axarr[i,j].set_yticks(arange(0,1.1,0.1))
                      axarr[i,j].set_yticklabels(range(0,110,10))
                      axarr[i,j].set_xticklabels([h for h in hticks], rotation=45)
                  else:
                      axarr[i,j].axis('off')
                  k += 1
          plt.savefig(output['hsmetsub'][i_subplot])
      plt.close()

      # Write csv summary file
      pmsum = pm[report_cfg['picard']['summarycolumns']]
      with open(output['summarytable'], "w") as fh:
          fh.write(pmsum.metrics.summary(sep=","))

      # Finally write entire merged metrics data frame
      with open(output['metricstable'], "w") as fh:
          fh.write(pm.metrics.summary(raw=True, sep=","))


rule picard_qc_report:
    """Generate QC report for picard metrics data"""
    input: rules.report_generate_qc_plots.output
    output: os.path.join(report_cfg['reportdir'], 'picard_qc_report.html')
    run:
      kw = {
          'project_name' : report_cfg['project_name'],
          'application' : report_cfg['application']
      }
      reportdir = report_cfg['reportdir'] + os.sep if not report_cfg['reportdir'].endswith(os.sep) else report_cfg['reportdir']
      d = dict(rules.report_generate_qc_plots.output.items())
      i = 0
      for x in d['hsmetsub']:
          d['hsmetsub_' + str(i)] = x
          i += 1
      kw.update ([(k, str(v).replace(reportdir, '')) for k,v in d.items()])
      kw['hsmetsub'] = kw['hsmetsub'].split(" ")
      s = picard_qc_report.render(**kw)
      del d['hsmetsub']
      report(s, str(output), **d)

rule gatk_eval_report:
    """Generate variant summary report based on GATK VariantEval metrics"""
    input: "{prefix}.eval_metrics"
    output: report = os.path.join(report_cfg['reportdir'], "{prefix}.html"),
            variants_per_sample = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-variants-per-sample.png"),
            known_site_freq = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-known-site-freq.png"),
            dbsnp_concordance_known = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-dbsnp-concordance-known.png"),
            nSNP = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nSNP.png"),
            nIns = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nIns.png"),
            nDel = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nDel.png"),
            nComp = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nComp.png"),
            nMNP = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nMNP.png"),
            nHets = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nHets.png"),
            nHomRef = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nHomRef.png"),
            nHomVar = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nHomVar.png"),
            nNoCalls = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nNoCalls.png"),
            nSingletons = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-nSingletons.png"),
            TiTv = os.path.join(report_cfg['graphicsdir'], "{prefix}-gatk-eval-TiTv.png")
    run:
      R("""
      library(gsalib)
      library(lattice)

      # lattice.options(trellis.par.set(simpleTheme(pch=19)))
      
      ev <- gsa.read.gatkreport('{input}')

      # Plot results from CompOverlap
      x <- ev$CompOverlap
      x <- x[x$Sample != "all", ]
      
      png('{output.variants_per_sample}')
      print(stripplot(nEvalVariants ~ Sample | Filter + Novelty, data=x,
      subset=(Filter != "raw" & Novelty != "all"),
      scales=list(x=list(draw=FALSE)),
      main="Number of variants per sample\n(variants are classified as known/novel and called/filtered)",
      ylab="Number of variants", xlab="Sample"
      ))
      dev.off()

      png('{output.known_site_freq}')
      print(stripplot(compRate ~ Sample | Filter, data=x,
      subset=(Novelty == "all"), index.cond=list(c(3,1,2)),
      scales=list(x=list(draw=FALSE)), layout=c(3,1),
      main="Frequency of known sites",
      ylab="Known sites (%)", xlab="Sample",
      ))
      dev.off()

      png('{output.dbsnp_concordance_known}')
      print(stripplot(concordantRate ~ Sample | Filter, data=x,
                subset=(Novelty == "known"), index.cond=list(c(3,1,2)),
                scales=list(x=list(draw=FALSE)), layout=c(3,1),
                main="Concordance with dbSNP for variants at known sites",
                ylab="Concordant variants (%)", xlab="Sample"
                ))
      dev.off()

      # Plot results from CountVariants
      x <- ev$CountVariants
      x <- x[x$Sample != "all", ]


      count.metrics <- list(nSNP = c("Number of SNPs", "nSNPs", '{output.nSNP}'),
      nIns = c("Number of insertions", "nInsertions", '{output.nIns}'),
      nDel = c("Number of deletions", "nDeletions", '{output.nDel}'),
      nComp = c("Number of complex variants", "nComplex", '{output.nComp}'),
      nMNP = c("Number of other variants (MNP, symbolic and mixed)", "I(nMNPs + nSymbolic + nMixed)", '{output.nMNP}'),
      nHets = c("Number of heterozygous sites", "nHets", '{output.nHets}'),
      nHomRef = c("Number of homozygous-reference sites", "nHomRef", '{output.nHomRef}'),
      nHomVar = c("Number of homozygous-alternative sites", "nHomVar", '{output.nHomVar}'),
      nNoCalls = c("Number of 'no calls'", "nNoCalls", '{output.nNoCalls}'),
      nSingletons = c("Number of private variants", "nSingletons", '{output.nSingletons}')
      )

      for(metric.name in names(count.metrics)) {{
      png(count.metrics[[metric.name]][3])
      print(stripplot(as.formula(paste(count.metrics[[metric.name]][2], "~ Sample | Filter + Novelty")),
      data=x, subset=(Filter != "raw" & Novelty != "all"),
      scales=list(x=list(draw=FALSE)),
      main=paste(metric.name, "per sample"), ylab=count.metrics[[metric.name]][1], xlab="Sample"
      ))
      dev.off()
      }}

      ## Plot results from TiTvVariantEvaluator

      x <- ev$TiTvVariantEvaluator
      x <- x[x$Sample != "all", ]
      
      png('{output.TiTv}')
      print(stripplot(tiTvRatio ~ Sample | Filter + Novelty, data=x,
      subset=(Filter != "raw" & Novelty != "all"),
      scales=list(x=list(draw=FALSE)),
      main="Transition / transversion ratio for each sample",
      ylab="Transition / transversion ratio", xlab="Sample"
      ))
      dev.off()

    """)
      reportdir = report_cfg['reportdir'] + os.sep if not report_cfg['reportdir'].endswith(os.sep) else report_cfg['reportdir']
      kw = {
          'project_name' : report_cfg['project_name'],
          'application' : report_cfg['application'],
          'input' : input,
      }
      kw.update({k:v.replace(reportdir, '') for k,v in dict(output).items()})
      d = dict(output.items())
      s = gatk_eval_report.render(**kw)
      del d['report']
      report(s, str(output.report), **d)


rule report_rulegraphs:
    """Generate rulegraphs for targets specified by report_cfg['rulegraphs']"""
    input: [os.path.join(report_cfg['reportdir'], x) for x in report_cfg['rulegraphs']]
    output: os.path.join(report_cfg['reportdir'], "rules.html")
    run:
      kw = {
          'project_name' : report_cfg['project_name'],
          'application' : report_cfg['application'],
          'rulegraphs' : [x for x in report_cfg['rulegraphs']],
      }
      d = {}
      d.update([(x,os.path.join(report_cfg['reportdir'], x)) for x in report_cfg['rulegraphs']])
      s = rule_report.render(**kw)
      report(s, str(output), **d)
      

rule sample_dag:
    """Utility rule to generate dag for first sample in sample list"""
    input: "{prefix}", cfg['bio.ngs.settings']['samples'][0]
    output: "{prefix}_dag.png"
    shell: "snakemake  --dag {input[0]} --config samples=\"['{input[1]}']\" | dot -Tpng -o {output}"

rule rulegraph:
    """Generate rulegraph for a specific rule"""
    input: "{prefix}"
    output: "{prefix}.png"
    shell: "snakemake -F --rulegraph {input} | dot -Tpng -o {output}"

ruleorder: rulegraph > sample_dag
